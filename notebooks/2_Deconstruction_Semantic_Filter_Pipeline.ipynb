{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"colab":{"provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"1b69543e7b6f4639a675c6e5f79bf57f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0eb8c9c43dec4623b3542519572a0ead","IPY_MODEL_9f0097098e46401bba20adc7edfe694a","IPY_MODEL_74f6ddea6ce545e2a304b528eef252f9"],"layout":"IPY_MODEL_6f1f3f7c3c5549c2b3431d5d2ea8d51a"}},"0eb8c9c43dec4623b3542519572a0ead":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_77b2b8ccc5734d65b324f6a3aeca09d3","placeholder":"​","style":"IPY_MODEL_9d53b71a3a8b4559af84402d3026b629","value":"Processing PDFs: 100%"}},"9f0097098e46401bba20adc7edfe694a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0add9daf3e244aeb93fc9c2e74785e8e","max":95,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fbebb4c88782405dbc42cf6d8801b4ab","value":95}},"74f6ddea6ce545e2a304b528eef252f9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9dc0b6872cd54b64ba16dd014998aead","placeholder":"​","style":"IPY_MODEL_f74b048df0a14af684f08dc851a88452","value":" 95/95 [1:24:33&lt;00:00, 15.46s/it]"}},"6f1f3f7c3c5549c2b3431d5d2ea8d51a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77b2b8ccc5734d65b324f6a3aeca09d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d53b71a3a8b4559af84402d3026b629":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0add9daf3e244aeb93fc9c2e74785e8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbebb4c88782405dbc42cf6d8801b4ab":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9dc0b6872cd54b64ba16dd014998aead":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f74b048df0a14af684f08dc851a88452":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6e7cb9d4a67e429c8690bc3e018d5bbc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d99164bfaba543d283a25539adef0612","IPY_MODEL_02d16ecfd71040269c32a2e0d4cd3fa3","IPY_MODEL_51d799057fb347428deeb5ef38f058be"],"layout":"IPY_MODEL_61116f52dc4e480cb510c0d9761ad9d0"}},"d99164bfaba543d283a25539adef0612":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2eaff001245547db8e850633fbd7e16e","placeholder":"​","style":"IPY_MODEL_862b102255c64c91bb2855186d28f44a","value":"Judging chunks: 100%"}},"02d16ecfd71040269c32a2e0d4cd3fa3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_15a126e9899e4bfab8b37c4ad050ec3e","max":364,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7f8cff90a066418b87093072aa3dc680","value":364}},"51d799057fb347428deeb5ef38f058be":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3aea7ee7a9014223a5b25df32b9d64a2","placeholder":"​","style":"IPY_MODEL_fc2c909c804d4b0cb8ea04b28dfab188","value":" 364/364 [06:48&lt;00:00,  1.14s/it]"}},"61116f52dc4e480cb510c0d9761ad9d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2eaff001245547db8e850633fbd7e16e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"862b102255c64c91bb2855186d28f44a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"15a126e9899e4bfab8b37c4ad050ec3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f8cff90a066418b87093072aa3dc680":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3aea7ee7a9014223a5b25df32b9d64a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc2c909c804d4b0cb8ea04b28dfab188":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"H0lN31BbA_4c"},"source":["# Deconstruction Policy Corpus -- Semantic Filtering Pipeline\n","\n","**Purpose.** This notebook implements the data pre-processing and semantic\n","filtering stages (Section 3.1) and the optional LLM-judge precision filter (Section 3.2)\n","described in the paper's Methods section.  It processes a heterogeneous\n","corpus of Australian policy PDFs (*n* = 95) through the following stages:\n","\n","1. **Page-level text extraction** with multi-engine fallback and OCR\n","   (`pdfplumber` -> `PyMuPDF` -> `Tesseract OCR`).\n","2. **Explicit table extraction** to recover tabular content that\n","   standard text extraction may miss or linearise incorrectly.\n","3. **Text segmentation** into overlapping analysis units with page\n","   provenance (LangChain `RecursiveCharacterTextSplitter`).\n","4. **Keyword gating** for high-recall pre-screening.\n","5. **Vector embedding and FAISS similarity search** with calibrated\n","   relevance scoring.\n","6. **Optional LLM judge** for precision filtering.\n","\n","### Outputs (written under `OUT_DIR/outputs/`)\n","\n","| Sub-folder | File | Description |\n","|---|---|---|\n","| `full_text/` | `<pdf>.pages.jsonl` | Page-by-page full text with extraction metadata and quality flags |\n","| `kept_chunks/` | `<pdf>.kept.jsonl` | Semantic-filtered chunks per PDF |\n","| `kept_chunks/` | `MASTER_kept.jsonl` | Aggregated kept chunks across the corpus |\n","| `kept_chunks/` | `MASTER_kept_judged.jsonl` | After optional LLM judge |\n","| `faiss/` | `<pdf>/` | Per-PDF FAISS index (for re-querying without re-embedding) |\n","\n","> **Note on OCR:** Pages with very low extracted text (likely scanned/image-only)\n","> are automatically processed with Tesseract OCR as a fallback.  Pages that\n","> remain low-text after all extraction attempts are flagged in the JSONL output."],"id":"H0lN31BbA_4c"},{"cell_type":"markdown","metadata":{"id":"IZW7L8p3A_4f"},"source":["## 0) Prerequisites\n","\n","- Run in **Google Colab** (GPU not required; CPU is sufficient).\n","- Store the PDF corpus in a Google Drive folder.\n","- Add your **LLM API key** in Colab -> **Settings -> Secrets**.\n","  - For OpenAI models: add as `OPENAI_API_KEY`\n","  - For Google Gemini models: add as `GOOGLE_API_KEY`\n","  - For Anthropic Claude models: add as `ANTHROPIC_API_KEY`"],"id":"IZW7L8p3A_4f"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2L5WvS5aA_4f","executionInfo":{"status":"ok","timestamp":1770785418236,"user_tz":-660,"elapsed":56941,"user":{"displayName":"M. Reza Hosseini","userId":"13449621777993109619"}},"outputId":"1b043132-8213-4ee0-a455-349a70089d04"},"source":["# @title 1) Install & Imports (run once)\n","!pip install -q langchain langchain-openai langchain-google-genai langchain-anthropic \\\n","    langchain-community langchain-text-splitters \\\n","    pypdf tiktoken faiss-cpu pandas numpy tqdm \\\n","    pdfplumber PyMuPDF pytesseract pdf2image\n","\n","# Tesseract OCR engine + Poppler (PDF-to-image renderer)\n","!apt-get -qq install -y tesseract-ocr poppler-utils > /dev/null 2>&1\n","\n","import os, re, json, time, math, hashlib, logging, warnings\n","from pathlib import Path\n","from typing import List, Dict, Any, Optional, Tuple\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm.auto import tqdm\n","\n","from google.colab import drive, userdata\n","\n","import pdfplumber\n","import fitz  # PyMuPDF\n","import pytesseract\n","from pdf2image import convert_from_path\n","\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","from langchain_community.vectorstores import FAISS\n","\n","# Suppress noisy pdfminer warnings (non-fatal ToUnicode CMap issues)\n","logging.getLogger(\"pdfminer\").setLevel(logging.ERROR)\n","warnings.filterwarnings(\"ignore\", message=\".*ToUnicode.*\")\n","\n","print(\"All imports OK. Tesseract version:\", pytesseract.get_tesseract_version())"],"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.8/85.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.6/330.6 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.9/405.9 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m500.1/500.1 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mAll imports OK. Tesseract version: 4.1.1\n"]}],"execution_count":1,"id":"2L5WvS5aA_4f"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tE0Ybm4vA_4g","executionInfo":{"status":"ok","timestamp":1770785464098,"user_tz":-660,"elapsed":16457,"user":{"displayName":"M. Reza Hosseini","userId":"13449621777993109619"}},"outputId":"c66b1f8d-b698-4b05-e54a-3709a38ec734"},"source":["# @title 2) Mount Drive + Set Paths (EDIT THESE)\n","drive.mount('/content/drive')\n","\n","PDF_DIR = \"/content/drive/MyDrive/ACTIVE/AU_deconstruction_domain/Governance\"   # <-- EDIT\n","OUT_DIR = \"/content/drive/MyDrive/ACTIVE/AU_deconstruction_domain/data_analysis/batch_enhanced_KG_outputs\"  # <-- EDIT\n","\n","Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n","print(\"PDF_DIR:\", PDF_DIR)\n","print(\"OUT_DIR:\", OUT_DIR)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","PDF_DIR: /content/drive/MyDrive/ACTIVE/AU_deconstruction_domain/Governance\n","OUT_DIR: /content/drive/MyDrive/ACTIVE/AU_deconstruction_domain/data_analysis/batch_enhanced_KG_outputs\n"]}],"execution_count":2,"id":"tE0Ybm4vA_4g"},{"cell_type":"markdown","metadata":{"id":"51U0iUwJA_4g"},"source":["## 3) Configuration\n","\n","### 3A) LLM Provider and Model Selection\n","\n","The pipeline uses LLMs at two points: **embeddings** (for FAISS indexing)\n","and an optional **judge** (for precision filtering).  The cell below lets\n","you select your provider and model for each.  Currently supported:\n","\n","| Provider | Embedding models | Chat/Judge models | API key secret name |\n","|---|---|---|---|\n","| **OpenAI** | `text-embedding-3-small`, `text-embedding-3-large` | `gpt-4o-mini`, `gpt-4o`, `gpt-4.1-mini-2025-04-14`, `gpt-4.1-2025-04-14` | `OPENAI_API_KEY` |\n","| **Google Gemini** | `models/text-embedding-004` | `gemini-2.0-flash`, `gemini-2.5-flash-preview-05-20`, `gemini-2.5-pro-preview-05-06` | `GOOGLE_API_KEY` |\n","| **Anthropic Claude** | *(use OpenAI or Google for embeddings)* | `claude-sonnet-4-20250514`, `claude-haiku-4-5-20241022` | `ANTHROPIC_API_KEY` |\n","\n","> **Mixed providers:** You can use one provider for embeddings and a different\n","> one for the judge (e.g., OpenAI embeddings + Gemini judge).\n","\n","### 3B) Chunking, Thresholds, and Retrieval\n","\n","| Parameter | Value | Rationale |\n","|---|---|---|\n","| Chunk size | 1 400 chars | Preserves legal definitions and qualification clauses |\n","| Chunk overlap | 200 chars | Maintains cross-reference context at boundaries |\n","| FAISS normalisation | `normalize_L2=True` | Enables stable L2 -> cosine conversion |\n","| Relevance score | `1 - d^2/2`, clipped to [0, 1] | Bounded cosine-derived metric |\n","| Relevance threshold | 0.55 | Calibrated via pilot runs for recall/precision balance |\n","| Top-k per query | 25 | Balances retrieval depth with precision |"],"id":"51U0iUwJA_4g"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pCaAEOnGA_4g","executionInfo":{"status":"ok","timestamp":1770785566365,"user_tz":-660,"elapsed":6007,"user":{"displayName":"M. Reza Hosseini","userId":"13449621777993109619"}},"outputId":"5345a019-fa65-4a90-c6db-8cad82179138"},"source":["# @title 3A) LLM Provider & Model Selection\n","# ============================================================\n","# EDIT THESE to switch between providers and models.\n","# ============================================================\n","\n","# -- Embedding provider -------------------------------------------\n","# Options: \"openai\" or \"google\"\n","EMBED_PROVIDER = \"openai\"\n","\n","# OpenAI embedding models: \"text-embedding-3-small\", \"text-embedding-3-large\"\n","# Google embedding models: \"models/text-embedding-004\"\n","EMBED_MODEL = \"text-embedding-3-large\"\n","\n","# -- Judge / Chat LLM provider -----------------------------------\n","# Options: \"openai\", \"google\", or \"anthropic\"\n","JUDGE_PROVIDER = \"openai\"\n","\n","# OpenAI:    \"gpt-4o-mini\", \"gpt-4o\", \"gpt-4.1-mini-2025-04-14\", \"gpt-4.1-2025-04-14\"\n","# Google:    \"gemini-2.0-flash\", \"gemini-2.5-flash-preview-05-20\", \"gemini-2.5-pro-preview-05-06\"\n","# Anthropic: \"claude-sonnet-4-20250514\", \"claude-haiku-4-5-20241022\"\n","JUDGE_MODEL = \"gpt-4.1-2025-04-14\"\n","\n","# -- Pipeline toggles ---------------------------------------------\n","USE_LLM_JUDGE = True\n","\n","# ============================================================\n","# API key loading (auto-selects based on provider choices above)\n","# ============================================================\n","_required_keys = set()\n","if EMBED_PROVIDER == \"openai\" or JUDGE_PROVIDER == \"openai\":\n","    _required_keys.add(\"OPENAI_API_KEY\")\n","if EMBED_PROVIDER == \"google\" or JUDGE_PROVIDER == \"google\":\n","    _required_keys.add(\"GOOGLE_API_KEY\")\n","if JUDGE_PROVIDER == \"anthropic\":\n","    _required_keys.add(\"ANTHROPIC_API_KEY\")\n","\n","_api_keys = {}\n","for key_name in _required_keys:\n","    val = userdata.get(key_name)\n","    if not val:\n","        raise ValueError(f\"Missing {key_name}. Add it in Colab: Settings -> Secrets -> {key_name}\")\n","    _api_keys[key_name] = val\n","    os.environ[key_name] = val\n","\n","# -- Instantiate embedding model ----------------------------------\n","if EMBED_PROVIDER == \"openai\":\n","    from langchain_openai import OpenAIEmbeddings\n","    embeddings = OpenAIEmbeddings(\n","        model=EMBED_MODEL,\n","        api_key=_api_keys[\"OPENAI_API_KEY\"]\n","    )\n","elif EMBED_PROVIDER == \"google\":\n","    from langchain_google_genai import GoogleGenerativeAIEmbeddings\n","    embeddings = GoogleGenerativeAIEmbeddings(\n","        model=EMBED_MODEL,\n","        google_api_key=_api_keys[\"GOOGLE_API_KEY\"]\n","    )\n","else:\n","    raise ValueError(f\"Unsupported EMBED_PROVIDER: {EMBED_PROVIDER}\")\n","\n","# -- Instantiate judge LLM (if enabled) ---------------------------\n","judge_llm = None\n","if USE_LLM_JUDGE:\n","    if JUDGE_PROVIDER == \"openai\":\n","        from langchain_openai import ChatOpenAI\n","        judge_llm = ChatOpenAI(\n","            model=JUDGE_MODEL,\n","            api_key=_api_keys[\"OPENAI_API_KEY\"],\n","            temperature=0\n","        )\n","    elif JUDGE_PROVIDER == \"google\":\n","        from langchain_google_genai import ChatGoogleGenerativeAI\n","        judge_llm = ChatGoogleGenerativeAI(\n","            model=JUDGE_MODEL,\n","            google_api_key=_api_keys[\"GOOGLE_API_KEY\"],\n","            temperature=0\n","        )\n","    elif JUDGE_PROVIDER == \"anthropic\":\n","        from langchain_anthropic import ChatAnthropic\n","        judge_llm = ChatAnthropic(\n","            model=JUDGE_MODEL,\n","            api_key=_api_keys[\"ANTHROPIC_API_KEY\"],\n","            temperature=0\n","        )\n","    else:\n","        raise ValueError(f\"Unsupported JUDGE_PROVIDER: {JUDGE_PROVIDER}\")\n","\n","print(f\"Embedding:  {EMBED_PROVIDER} / {EMBED_MODEL}\")\n","print(f\"Judge LLM:  {JUDGE_PROVIDER} / {JUDGE_MODEL} (enabled={USE_LLM_JUDGE})\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["Embedding:  openai / text-embedding-3-large\n","Judge LLM:  openai / gpt-4.1-2025-04-14 (enabled=True)\n"]}],"execution_count":3,"id":"pCaAEOnGA_4g"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"25XvhWV9A_4g","executionInfo":{"status":"ok","timestamp":1770785576641,"user_tz":-660,"elapsed":10,"user":{"displayName":"M. Reza Hosseini","userId":"13449621777993109619"}},"outputId":"0ae2fc40-c7e5-4092-cd0c-bd8fdccc8f26"},"source":["# @title 3B) Chunking, thresholds, and retrieval settings\n","CHUNK_SIZE = 1400\n","CHUNK_OVERLAP = 200\n","\n","USE_KEYWORD_GATE = True\n","\n","TOP_K_PER_QUERY = 25\n","RELEVANCE_THRESHOLD = 0.55\n","\n","JUDGE_KEEP_LABELS = {\"keep\", \"maybe\"}  # set to {\"keep\"} for stricter precision\n","\n","# -- PDF extraction settings --------------------------------------\n","# Minimum character count to consider a page as having usable text.\n","# Pages below this trigger fallback extraction (PyMuPDF -> OCR).\n","MIN_PAGE_TEXT_LEN = 50\n","\n","# Tesseract OCR language (install extra packs if needed)\n","OCR_LANG = \"eng\"\n","\n","# -- Resumability -------------------------------------------------\n","SKIP_ALREADY_PROCESSED = True\n","\n","# -- Retry settings for API calls ---------------------------------\n","MAX_RETRIES = 3\n","RETRY_BACKOFF = 5  # seconds, multiplied by attempt number\n","\n","print(\"Chunk:\", CHUNK_SIZE, \"/\", CHUNK_OVERLAP)\n","print(\"Keyword gate:\", USE_KEYWORD_GATE)\n","print(\"Relevance threshold:\", RELEVANCE_THRESHOLD)\n","print(\"Min page text length for OCR trigger:\", MIN_PAGE_TEXT_LEN)\n","print(\"Skip already processed:\", SKIP_ALREADY_PROCESSED)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Chunk: 1400 / 200\n","Keyword gate: True\n","Relevance threshold: 0.55\n","Min page text length for OCR trigger: 50\n","Skip already processed: True\n"]}],"execution_count":4,"id":"25XvhWV9A_4g"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S6m7LQMcA_4g","executionInfo":{"status":"ok","timestamp":1770785585305,"user_tz":-660,"elapsed":10,"user":{"displayName":"M. Reza Hosseini","userId":"13449621777993109619"}},"outputId":"aab8ed94-f488-4449-9717-d33131c53b3f"},"source":["# @title 3C) Queries and keyword patterns (EDIT as needed)\n","FILTER_QUERIES = [\n","    \"building deconstruction and disassembly methods\",\n","    \"selective deconstruction versus demolition for material recovery\",\n","    \"design for deconstruction (DfD) requirements and guidance\",\n","    \"reuse of secondary construction materials from deconstruction\",\n","    \"salvage, recovery, and reuse pathways in building deconstruction\",\n","    \"circular economy policy instruments related to deconstruction and reuse\",\n","]\n","\n","# High-recall keyword patterns (regex)\n","KEYWORD_PATTERNS = [\n","    r\"\\bdeconstruct(ion|ing)?\\b\",\n","    r\"\\bselective\\s+demolition\\b\",\n","    r\"\\bsoft\\s+strip(ping)?\\b\",\n","    r\"\\bdisassembl(y|e|ing)\\b\",\n","    r\"\\bdesign\\s+for\\s+deconstruction\\b\",\n","    r\"\\b(material|component)\\s+reuse\\b\",\n","    r\"\\bsalvag(e|ing)\\b\",\n","    r\"\\brecover(y|ed)\\b\",\n","    r\"\\bsecondary\\s+materials?\\b\",\n","    r\"\\bcircular\\s+econom(y|ic)\\b\",\n","]\n","KW_REGEX = re.compile(\"|\".join(KEYWORD_PATTERNS), re.IGNORECASE)\n","\n","print(\"Queries:\", len(FILTER_QUERIES), \"| Keyword patterns:\", len(KEYWORD_PATTERNS))"],"outputs":[{"output_type":"stream","name":"stdout","text":["Queries: 6 | Keyword patterns: 10\n"]}],"execution_count":5,"id":"S6m7LQMcA_4g"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OMk-wrm_A_4h","executionInfo":{"status":"ok","timestamp":1770785589537,"user_tz":-660,"elapsed":46,"user":{"displayName":"M. Reza Hosseini","userId":"13449621777993109619"}},"outputId":"d1d54598-6d08-4910-b6bd-bdfb5551572e"},"source":["# @title 4) Helper functions\n","\n","# ==================================================================\n","#  Utility helpers\n","# ==================================================================\n","\n","def safe_slug(name: str) -> str:\n","    \"\"\"Convert a filename to a filesystem-safe slug (max 180 chars).\"\"\"\n","    s = re.sub(r\"[^\\w\\-\\.]+\", \"_\", name.strip())\n","    return s[:180]\n","\n","def sha1(text: str) -> str:\n","    \"\"\"Return the SHA-1 hex digest of a text string (for chunk dedup).\"\"\"\n","    return hashlib.sha1(text.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n","\n","def list_pdfs(pdf_dir: str) -> List[str]:\n","    \"\"\"Recursively list all PDF files under a directory.\"\"\"\n","    p = Path(pdf_dir)\n","    return sorted([str(x) for x in p.glob(\"**/*.pdf\")])\n","\n","def call_with_retry(fn, *args, max_retries=MAX_RETRIES, backoff=RETRY_BACKOFF, **kwargs):\n","    \"\"\"Call fn() with exponential-backoff retry on transient API errors.\"\"\"\n","    for attempt in range(1, max_retries + 1):\n","        try:\n","            return fn(*args, **kwargs)\n","        except Exception as e:\n","            err_str = str(e).lower()\n","            is_transient = any(kw in err_str for kw in [\n","                \"rate limit\", \"rate_limit\", \"429\", \"timeout\",\n","                \"connection\", \"server error\", \"500\", \"502\", \"503\",\n","                \"overloaded\", \"resource_exhausted\"\n","            ])\n","            if is_transient and attempt < max_retries:\n","                wait = backoff * attempt\n","                print(f\"  Retry {attempt}/{max_retries} after {wait}s -- {e}\")\n","                time.sleep(wait)\n","            else:\n","                raise\n","\n","\n","# ==================================================================\n","#  PDF extraction: multi-engine with OCR fallback\n","# ==================================================================\n","\n","def _extract_text_pdfplumber(pdf_path: str, page_num: int) -> Tuple[str, str]:\n","    \"\"\"Extract text from a single page using pdfplumber.\n","\n","    Also attempts explicit table extraction: pdfplumber can miss the\n","    rightmost column of wide tables or linearise multi-column layouts\n","    incorrectly.  We extract tables separately and append their content\n","    to capture data that body-text extraction may drop.\n","\n","    Returns (text, method_used).\n","    \"\"\"\n","    try:\n","        with pdfplumber.open(pdf_path) as pdf:\n","            if page_num - 1 >= len(pdf.pages):\n","                return (\"\", \"pdfplumber_out_of_range\")\n","            page = pdf.pages[page_num - 1]\n","\n","            # Body text\n","            body = page.extract_text() or \"\"\n","\n","            # Explicit table extraction (compensates for dropped columns)\n","            table_text_parts = []\n","            try:\n","                tables = page.extract_tables()\n","                for table in (tables or []):\n","                    for row in table:\n","                        cleaned = [str(cell).strip() if cell else \"\" for cell in row]\n","                        table_text_parts.append(\" | \".join(cleaned))\n","            except Exception:\n","                pass  # table extraction is best-effort\n","\n","            # Merge: append table text only if it adds content not in body\n","            table_block = \"\\n\".join(table_text_parts)\n","            if table_block and len(table_block.strip()) > 20:\n","                combined = body + \"\\n\\n[TABLE CONTENT]\\n\" + table_block\n","            else:\n","                combined = body\n","\n","            return (combined, \"pdfplumber\")\n","    except Exception as e:\n","        return (\"\", f\"pdfplumber_error:{e}\")\n","\n","\n","def _extract_text_pymupdf(pdf_path: str, page_num: int) -> Tuple[str, str]:\n","    \"\"\"Fallback text extraction using PyMuPDF (fitz).\n","\n","    PyMuPDF uses a different PDF parser (MuPDF) and often handles\n","    encoding edge-cases, complex layouts, and embedded fonts that\n","    pdfplumber/pdfminer struggle with.\n","    \"\"\"\n","    try:\n","        doc = fitz.open(pdf_path)\n","        if page_num - 1 >= len(doc):\n","            doc.close()\n","            return (\"\", \"pymupdf_out_of_range\")\n","        page = doc[page_num - 1]\n","        text = page.get_text(\"text\") or \"\"\n","        doc.close()\n","        return (text, \"pymupdf\")\n","    except Exception as e:\n","        return (\"\", f\"pymupdf_error:{e}\")\n","\n","\n","def _extract_text_ocr(pdf_path: str, page_num: int) -> Tuple[str, str]:\n","    \"\"\"Last-resort OCR extraction using Tesseract via pdf2image.\n","\n","    Converts the target page to an image, then runs Tesseract OCR.\n","    Suitable for scanned / image-only pages.\n","    \"\"\"\n","    try:\n","        images = convert_from_path(\n","            pdf_path,\n","            first_page=page_num,\n","            last_page=page_num,\n","            dpi=300\n","        )\n","        if not images:\n","            return (\"\", \"ocr_no_image\")\n","        text = pytesseract.image_to_string(images[0], lang=OCR_LANG)\n","        return (text or \"\", \"ocr_tesseract\")\n","    except Exception as e:\n","        return (\"\", f\"ocr_error:{e}\")\n","\n","\n","def extract_pages(pdf_path: str) -> List[Dict[str, Any]]:\n","    \"\"\"Extract text from every page of a PDF using a three-tier strategy:\n","\n","    Tier 1: pdfplumber (with explicit table extraction)\n","    Tier 2: PyMuPDF (different parser; handles some encodings better)\n","    Tier 3: Tesseract OCR (for scanned / image-only pages)\n","\n","    Each page record includes:\n","    - page_num, text, text_len\n","    - extraction_method: which engine produced the final text\n","    - fallback_attempted: whether fallback engines were tried\n","    - likely_image_or_scan: flagged if text remains very short after all attempts\n","    \"\"\"\n","    # Determine page count via PyMuPDF (fast, reliable)\n","    try:\n","        doc = fitz.open(pdf_path)\n","        n_pages = len(doc)\n","        doc.close()\n","    except Exception as e:\n","        print(f\"  Cannot open PDF {os.path.basename(pdf_path)}: {e}\")\n","        return [{\"page_num\": 1, \"text\": \"\", \"text_len\": 0,\n","                 \"extraction_method\": \"failed\", \"fallback_attempted\": False,\n","                 \"likely_image_or_scan\": True}]\n","\n","    pages = []\n","    for pg in range(1, n_pages + 1):\n","        text = \"\"\n","        method = \"\"\n","        fallback = False\n","\n","        # Tier 1: pdfplumber (with table extraction)\n","        text, method = _extract_text_pdfplumber(pdf_path, pg)\n","\n","        # Tier 2: PyMuPDF fallback if pdfplumber returned very little\n","        if len(text.strip()) < MIN_PAGE_TEXT_LEN:\n","            fallback = True\n","            text2, method2 = _extract_text_pymupdf(pdf_path, pg)\n","            if len(text2.strip()) > len(text.strip()):\n","                text, method = text2, method2\n","\n","        # Tier 3: OCR fallback if still very little text\n","        if len(text.strip()) < MIN_PAGE_TEXT_LEN:\n","            fallback = True\n","            text3, method3 = _extract_text_ocr(pdf_path, pg)\n","            if len(text3.strip()) > len(text.strip()):\n","                text, method = text3, method3\n","\n","        # Flag pages that remain low-text after all attempts\n","        is_low = len(text.strip()) < MIN_PAGE_TEXT_LEN\n","\n","        pages.append({\n","            \"page_num\": pg,\n","            \"text\": text,\n","            \"text_len\": len(text),\n","            \"extraction_method\": method,\n","            \"fallback_attempted\": fallback,\n","            \"likely_image_or_scan\": is_low,\n","        })\n","\n","    return pages\n","\n","\n","# ==================================================================\n","#  JSONL I/O\n","# ==================================================================\n","\n","def save_pages_jsonl(pages: List[Dict[str, Any]], out_path: str, source_file: str) -> None:\n","    \"\"\"Write page records to JSONL (one JSON object per page).\"\"\"\n","    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n","    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n","        for p in pages:\n","            rec = {\n","                \"source_file\": source_file,\n","                \"page_num\": p[\"page_num\"],\n","                \"text\": p[\"text\"],\n","                \"text_len\": p[\"text_len\"],\n","                \"extraction_method\": p.get(\"extraction_method\", \"unknown\"),\n","                \"fallback_attempted\": p.get(\"fallback_attempted\", False),\n","                \"likely_image_or_scan\": p.get(\"likely_image_or_scan\", False),\n","            }\n","            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n","\n","\n","def save_jsonl(records: List[Dict[str, Any]], out_path: str) -> None:\n","    \"\"\"Write a list of dicts to a JSONL file (overwrite).\"\"\"\n","    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n","    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n","        for r in records:\n","            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n","\n","\n","def append_jsonl(records: List[Dict[str, Any]], out_path: str) -> None:\n","    \"\"\"Append a list of dicts to a JSONL file.\"\"\"\n","    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n","    with open(out_path, \"a\", encoding=\"utf-8\") as f:\n","        for r in records:\n","            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n","\n","\n","# ==================================================================\n","#  Chunking, keyword gating, relevance scoring\n","# ==================================================================\n","\n","def chunk_pages(pages: List[Dict[str, Any]], chunk_size: int, chunk_overlap: int) -> List[Dict[str, Any]]:\n","    \"\"\"Concatenate pages with [PAGE x] markers, split into overlapping\n","    chunks, and map each chunk back to its source page (best-effort).\"\"\"\n","    parts = []\n","    for p in pages:\n","        parts.append(f\"\\n\\n[PAGE {p['page_num']}]\\n\")\n","        parts.append(p[\"text\"] or \"\")\n","    full_text = \"\".join(parts)\n","\n","    splitter = RecursiveCharacterTextSplitter(\n","        chunk_size=chunk_size,\n","        chunk_overlap=chunk_overlap,\n","        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n","    )\n","    raw_chunks = splitter.split_text(full_text)\n","\n","    marker = re.compile(r\"\\[PAGE\\s+(\\d+)\\]\")\n","    chunks = []\n","    for idx, ch in enumerate(raw_chunks):\n","        m = marker.search(ch)\n","        page_num = int(m.group(1)) if m else None\n","        chunks.append({\"chunk_index\": idx, \"page_num\": page_num, \"text\": ch.strip()})\n","    return chunks\n","\n","\n","def keyword_gate(chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n","    \"\"\"Retain only chunks matching the keyword regex (high-recall filter).\"\"\"\n","    return [c for c in chunks if KW_REGEX.search(c[\"text\"])]\n","\n","\n","def l2_to_relevance(l2_dist: float) -> float:\n","    \"\"\"Convert FAISS L2 distance (normalised vectors) to [0, 1] relevance.\n","    With L2-normalised vectors: cos_sim = 1 - d^2 / 2.\"\"\"\n","    d = float(l2_dist)\n","    cos = 1.0 - (d * d) / 2.0\n","    return max(0.0, min(1.0, cos))\n","\n","\n","print(\"Helper functions defined.\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["Helper functions defined.\n"]}],"execution_count":6,"id":"OMk-wrm_A_4h"},{"cell_type":"markdown","metadata":{"id":"KP1Q1DxyA_4h"},"source":["## 5) Batch Processing -- Extract Full Text + Find Deconstruction Passages\n","\n","This step loops over every PDF in the corpus.  For each document it:\n","\n","1. **Extracts full text** page-by-page using the three-tier strategy\n","   (pdfplumber -> PyMuPDF -> Tesseract OCR), with explicit table extraction\n","   and quality flags -> `full_text/<pdf>.pages.jsonl`\n","2. **Splits** text into overlapping chunks (Section 3.1.2)\n","3. **Applies keyword gating** for high-recall screening (Section 3.1.3)\n","4. **Embeds** gated chunks and builds a per-document FAISS index (Section 3.1.4)\n","5. **Queries** the index with the construct-operationalising query set\n","6. **Retains** chunks above the calibrated relevance threshold -> `kept_chunks/`\n","7. **Appends** kept chunks to the master file `MASTER_kept.jsonl`\n","\n","**Robustness:** Each PDF is processed inside a `try/except` block.\n","If a single document fails, the pipeline logs the failure and continues.\n","A summary table and failure report are printed at the end.\n","\n","> **Tip:** For a quick pilot, uncomment `pdf_files = pdf_files[:3]` below."],"id":"KP1Q1DxyA_4h"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":2510,"referenced_widgets":["1b69543e7b6f4639a675c6e5f79bf57f","0eb8c9c43dec4623b3542519572a0ead","9f0097098e46401bba20adc7edfe694a","74f6ddea6ce545e2a304b528eef252f9","6f1f3f7c3c5549c2b3431d5d2ea8d51a","77b2b8ccc5734d65b324f6a3aeca09d3","9d53b71a3a8b4559af84402d3026b629","0add9daf3e244aeb93fc9c2e74785e8e","fbebb4c88782405dbc42cf6d8801b4ab","9dc0b6872cd54b64ba16dd014998aead","f74b048df0a14af684f08dc851a88452"]},"id":"sDNncvd9A_4h","executionInfo":{"status":"ok","timestamp":1770790672915,"user_tz":-660,"elapsed":5073629,"user":{"displayName":"M. Reza Hosseini","userId":"13449621777993109619"}},"outputId":"3009e3e5-95d7-452e-efdb-e4453eafc28c"},"source":["# @title 5) Run batch passage-finding (main)\n","pdf_files = list_pdfs(PDF_DIR)\n","if not pdf_files:\n","    raise FileNotFoundError(f\"No PDFs found under: {PDF_DIR}\")\n","print(\"PDFs found:\", len(pdf_files))\n","\n","# Uncomment for a pilot run:\n","# pdf_files = pdf_files[:3]\n","\n","full_text_dir = str(Path(OUT_DIR) / \"outputs\" / \"full_text\")\n","kept_dir      = str(Path(OUT_DIR) / \"outputs\" / \"kept_chunks\")\n","faiss_dir     = str(Path(OUT_DIR) / \"outputs\" / \"faiss\")\n","Path(full_text_dir).mkdir(parents=True, exist_ok=True)\n","Path(kept_dir).mkdir(parents=True, exist_ok=True)\n","Path(faiss_dir).mkdir(parents=True, exist_ok=True)\n","\n","master_kept_path = str(Path(kept_dir) / \"MASTER_kept.jsonl\")\n","if not SKIP_ALREADY_PROCESSED:\n","    open(master_kept_path, \"w\", encoding=\"utf-8\").close()\n","\n","run_stats = []\n","failed_pdfs = []\n","\n","for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n","    t0 = time.time()\n","    pdf_name = os.path.basename(pdf_path)\n","    stem = safe_slug(Path(pdf_name).stem)\n","\n","    # -- Resumability: skip if output already exists ---------------\n","    kept_out = str(Path(kept_dir) / f\"{stem}.kept.jsonl\")\n","    if SKIP_ALREADY_PROCESSED and Path(kept_out).exists():\n","        run_stats.append({\n","            \"pdf\": pdf_name, \"pages\": \"-\", \"chunks\": \"-\",\n","            \"kw_chunks\": \"-\", \"kept\": \"-\", \"ocr_pages\": \"-\",\n","            \"sec\": 0, \"status\": \"skipped\"\n","        })\n","        continue\n","\n","    try:\n","        # -- 1) Extract full text (page-by-page, multi-engine) ----\n","        pages = extract_pages(pdf_path)\n","        full_out = str(Path(full_text_dir) / f\"{stem}.pages.jsonl\")\n","        save_pages_jsonl(pages, full_out, source_file=pdf_name)\n","\n","        ocr_count = sum(1 for p in pages if p.get(\"fallback_attempted\"))\n","        flagged_count = sum(1 for p in pages if p.get(\"likely_image_or_scan\"))\n","\n","        # -- 2) Chunk ----------------------------------------------\n","        chunks = chunk_pages(pages, CHUNK_SIZE, CHUNK_OVERLAP)\n","\n","        # -- 3) Keyword gate ---------------------------------------\n","        chunks_for_embedding = keyword_gate(chunks) if USE_KEYWORD_GATE else chunks\n","\n","        if not chunks_for_embedding:\n","            save_jsonl([], kept_out)\n","            run_stats.append({\n","                \"pdf\": pdf_name, \"pages\": len(pages), \"chunks\": len(chunks),\n","                \"kw_chunks\": 0, \"kept\": 0, \"ocr_pages\": ocr_count,\n","                \"sec\": round(time.time() - t0, 1), \"status\": \"ok (no kw matches)\"\n","            })\n","            continue\n","\n","        texts = [c[\"text\"] for c in chunks_for_embedding]\n","        metadatas = [{\n","            \"source_file\": pdf_name,\n","            \"page_num\": c[\"page_num\"],\n","            \"chunk_index\": c[\"chunk_index\"],\n","            \"chunk_sha1\": sha1(c[\"text\"])[:16],\n","        } for c in chunks_for_embedding]\n","\n","        # -- 4) Build FAISS ----------------------------------------\n","        vs = call_with_retry(\n","            FAISS.from_texts, texts, embeddings,\n","            metadatas=metadatas, normalize_L2=True\n","        )\n","        pdf_faiss_out = str(Path(faiss_dir) / stem)\n","        vs.save_local(pdf_faiss_out)\n","\n","        # -- 5) Query and collect kept chunks ----------------------\n","        kept_map = {}\n","        for q in FILTER_QUERIES:\n","            results = call_with_retry(\n","                vs.similarity_search_with_score, q, k=TOP_K_PER_QUERY\n","            )\n","            for doc, score in results:\n","                rel = l2_to_relevance(score)\n","                if rel < RELEVANCE_THRESHOLD:\n","                    continue\n","\n","                md_dict = dict(doc.metadata)\n","                key = f\"{md_dict.get('chunk_index')}|{md_dict.get('chunk_sha1')}\"\n","                rec = {\n","                    \"chunk_id\": f\"{stem}::c{md_dict.get('chunk_index')}::{md_dict.get('chunk_sha1')}\",\n","                    \"source_file\": md_dict.get(\"source_file\"),\n","                    \"page_num\": md_dict.get(\"page_num\"),\n","                    \"chunk_index\": md_dict.get(\"chunk_index\"),\n","                    \"chunk_sha1\": md_dict.get(\"chunk_sha1\"),\n","                    \"matched_query\": q,\n","                    \"relevance\": round(float(rel), 4),\n","                    \"text\": doc.page_content,\n","                }\n","                if key not in kept_map or rec[\"relevance\"] > kept_map[key][\"relevance\"]:\n","                    kept_map[key] = rec\n","\n","        kept_chunks = sorted(\n","            kept_map.values(),\n","            key=lambda r: (-r[\"relevance\"], r[\"chunk_index\"])\n","        )\n","\n","        save_jsonl(kept_chunks, kept_out)\n","        append_jsonl(kept_chunks, master_kept_path)\n","\n","        run_stats.append({\n","            \"pdf\": pdf_name, \"pages\": len(pages), \"chunks\": len(chunks),\n","            \"kw_chunks\": len(chunks_for_embedding), \"kept\": len(kept_chunks),\n","            \"ocr_pages\": ocr_count,\n","            \"sec\": round(time.time() - t0, 1), \"status\": \"ok\"\n","        })\n","\n","    except Exception as e:\n","        failed_pdfs.append({\"pdf\": pdf_name, \"error\": str(e)})\n","        run_stats.append({\n","            \"pdf\": pdf_name, \"pages\": \"?\", \"chunks\": \"?\",\n","            \"kw_chunks\": \"?\", \"kept\": \"?\", \"ocr_pages\": \"?\",\n","            \"sec\": round(time.time() - t0, 1), \"status\": f\"FAILED: {e}\"\n","        })\n","        print(f\"  FAILED: {pdf_name} -- {e}\")\n","\n","# -- Summary -------------------------------------------------------\n","stats_df = pd.DataFrame(run_stats)\n","display(stats_df)\n","ok_count = len([s for s in run_stats if str(s.get(\"status\",\"\")).startswith(\"ok\")])\n","skip_count = len([s for s in run_stats if \"skipped\" in str(s.get(\"status\",\"\"))])\n","print(f\"\\nTotal: {len(pdf_files)} | OK: {ok_count} | Skipped: {skip_count} | Failed: {len(failed_pdfs)}\")\n","print(\"Master kept chunks:\", master_kept_path)\n","\n","if failed_pdfs:\n","    print(\"\\nFAILED PDFs:\")\n","    for fp in failed_pdfs:\n","        print(f\"  - {fp['pdf']}: {fp['error']}\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["PDFs found: 95\n"]},{"output_type":"display_data","data":{"text/plain":["Processing PDFs:   0%|          | 0/95 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b69543e7b6f4639a675c6e5f79bf57f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["                                                  pdf  pages  chunks  \\\n","0   1.national-waste-and-resource-recovery-report-...    104     298   \n","1                                  10.Report Dust.pdf      2       2   \n","2    11.Draft queenslands-waste-strategy2025-2030.pdf     18      56   \n","3                   12.NT Budget 2024-25-bp2-book.pdf    126     308   \n","4   13.Delivery Program 2020-2026 annual progress ...    104     521   \n","..                                                ...    ...     ...   \n","90  91.A case study of construction and demolition...     12      40   \n","91  92.Transformation towards a circular economy i...     58     110   \n","92  93.ACE_Hub_Circularity_In_Australian_Business_...     39      58   \n","93  94.Circular-Economy-Impact-Note_Final-for-Publ...      8      20   \n","94                  95.Circular Buildings Toolkit.pdf      5      10   \n","\n","    kw_chunks  kept  ocr_pages    sec              status  \n","0         215     2          0   65.0                  ok  \n","1           0     0          2   16.8  ok (no kw matches)  \n","2          18     0          1   32.6                  ok  \n","3           8     0          7  140.0                  ok  \n","4          26     0          3   70.0                  ok  \n","..        ...   ...        ...    ...                 ...  \n","90         11     4          0   11.0                  ok  \n","91         37    10          0   16.9                  ok  \n","92         47     0          1   19.8                  ok  \n","93         19     0          1   10.5                  ok  \n","94          3     0          0   12.5                  ok  \n","\n","[95 rows x 8 columns]"],"text/html":["\n","  <div id=\"df-42399c57-8572-4f9b-bb9a-aa15b43c7aec\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pdf</th>\n","      <th>pages</th>\n","      <th>chunks</th>\n","      <th>kw_chunks</th>\n","      <th>kept</th>\n","      <th>ocr_pages</th>\n","      <th>sec</th>\n","      <th>status</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.national-waste-and-resource-recovery-report-...</td>\n","      <td>104</td>\n","      <td>298</td>\n","      <td>215</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>65.0</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>10.Report Dust.pdf</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>16.8</td>\n","      <td>ok (no kw matches)</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>11.Draft queenslands-waste-strategy2025-2030.pdf</td>\n","      <td>18</td>\n","      <td>56</td>\n","      <td>18</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>32.6</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>12.NT Budget 2024-25-bp2-book.pdf</td>\n","      <td>126</td>\n","      <td>308</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>7</td>\n","      <td>140.0</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>13.Delivery Program 2020-2026 annual progress ...</td>\n","      <td>104</td>\n","      <td>521</td>\n","      <td>26</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>70.0</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>90</th>\n","      <td>91.A case study of construction and demolition...</td>\n","      <td>12</td>\n","      <td>40</td>\n","      <td>11</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>11.0</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>91</th>\n","      <td>92.Transformation towards a circular economy i...</td>\n","      <td>58</td>\n","      <td>110</td>\n","      <td>37</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>16.9</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>92</th>\n","      <td>93.ACE_Hub_Circularity_In_Australian_Business_...</td>\n","      <td>39</td>\n","      <td>58</td>\n","      <td>47</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>19.8</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>93</th>\n","      <td>94.Circular-Economy-Impact-Note_Final-for-Publ...</td>\n","      <td>8</td>\n","      <td>20</td>\n","      <td>19</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>10.5</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>94</th>\n","      <td>95.Circular Buildings Toolkit.pdf</td>\n","      <td>5</td>\n","      <td>10</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>12.5</td>\n","      <td>ok</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>95 rows × 8 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-42399c57-8572-4f9b-bb9a-aa15b43c7aec')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-42399c57-8572-4f9b-bb9a-aa15b43c7aec button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-42399c57-8572-4f9b-bb9a-aa15b43c7aec');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","  <div id=\"id_b75da0df-d17d-4dca-9c69-4acb4aa3377c\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('stats_df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_b75da0df-d17d-4dca-9c69-4acb4aa3377c button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('stats_df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"stats_df","summary":"{\n  \"name\": \"stats_df\",\n  \"rows\": 95,\n  \"fields\": [\n    {\n      \"column\": \"pdf\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 95,\n        \"samples\": [\n          \"71.The Reuse Playbook - Circular Buildings Toolkit.pdf\",\n          \"3. Resource Recovery and Waste materials analysis.pdf\",\n          \"75.Case-Study-1-1-ATCO-SL.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pages\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 53,\n        \"min\": 1,\n        \"max\": 342,\n        \"num_unique_values\": 59,\n        \"samples\": [\n          104,\n          11,\n          31\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunks\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 172,\n        \"min\": 1,\n        \"max\": 1055,\n        \"num_unique_values\": 70,\n        \"samples\": [\n          19,\n          298,\n          11\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"kw_chunks\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 67,\n        \"min\": 0,\n        \"max\": 331,\n        \"num_unique_values\": 46,\n        \"samples\": [\n          72,\n          1,\n          97\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"kept\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 0,\n        \"max\": 54,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          2,\n          8,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ocr_pages\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 0,\n        \"max\": 69,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          12,\n          17,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sec\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 96.67752218976509,\n        \"min\": 1.7,\n        \"max\": 698.3,\n        \"num_unique_values\": 91,\n        \"samples\": [\n          90.8,\n          131.3,\n          21.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"ok (no kw matches)\",\n          \"ok\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Total: 95 | OK: 95 | Skipped: 0 | Failed: 0\n","Master kept chunks: /content/drive/MyDrive/ACTIVE/AU_deconstruction_domain/data_analysis/batch_enhanced_KG_outputs/outputs/kept_chunks/MASTER_kept.jsonl\n"]}],"execution_count":7,"id":"sDNncvd9A_4h"},{"cell_type":"markdown","metadata":{"id":"6feUo9lpA_4h"},"source":["## 6) Optional: LLM Judge -- Precision Filter (Section 3.2)\n","\n","Each kept chunk is classified by the judge LLM as:\n","\n","- **`keep`** -- Directly about building deconstruction, disassembly, soft\n","  strip, or selective demolition for material recovery/reuse; or identifies\n","  formal definitions, requirements, responsible agencies/stakeholders,\n","  permitted/prohibited practices, certification pathways for reused materials,\n","  or documented barriers to deconstruction practice.\n","- **`maybe`** -- Partial or contextual relevance.\n","- **`drop`** -- Irrelevant.\n","\n","The judge uses deterministic decoding (`temperature=0`) and strict JSON output.\n","\n","Output: `outputs/kept_chunks/MASTER_kept_judged.jsonl`"],"id":"6feUo9lpA_4h"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":103,"referenced_widgets":["6e7cb9d4a67e429c8690bc3e018d5bbc","d99164bfaba543d283a25539adef0612","02d16ecfd71040269c32a2e0d4cd3fa3","51d799057fb347428deeb5ef38f058be","61116f52dc4e480cb510c0d9761ad9d0","2eaff001245547db8e850633fbd7e16e","862b102255c64c91bb2855186d28f44a","15a126e9899e4bfab8b37c4ad050ec3e","7f8cff90a066418b87093072aa3dc680","3aea7ee7a9014223a5b25df32b9d64a2","fc2c909c804d4b0cb8ea04b28dfab188"]},"id":"tcz7PInAA_4h","executionInfo":{"status":"ok","timestamp":1770791779793,"user_tz":-660,"elapsed":408441,"user":{"displayName":"M. Reza Hosseini","userId":"13449621777993109619"}},"outputId":"c935f73d-be38-4e76-e7fa-b7f527067628"},"source":["if USE_LLM_JUDGE and judge_llm is not None:\n","    in_path  = master_kept_path\n","    out_path = str(Path(kept_dir) / \"MASTER_kept_judged.jsonl\")\n","    open(out_path, \"w\", encoding=\"utf-8\").close()\n","\n","    JUDGE_PROMPT_TEMPLATE = (\n","        \"You are a precision filter for a research pipeline analysing Australian \"\n","        \"building deconstruction governance.\\n\\n\"\n","        \"Return JSON only with keys: label, confidence, reason.\\n\\n\"\n","        'label must be one of: \"keep\", \"maybe\", \"drop\"\\n'\n","        \"confidence must be a float between 0 and 1\\n\"\n","        \"reason must be <= 25 words.\\n\\n\"\n","        \"Definitions:\\n\"\n","        \"- keep: The passage directly concerns building deconstruction, disassembly, \"\n","        \"soft strip, or selective demolition for material recovery/reuse. This includes: \"\n","        \"formal definitions, legislative requirements, responsible agencies or stakeholders, \"\n","        \"permitted or prohibited practices, certification pathways for reused materials, \"\n","        \"or documented barriers (regulatory, economic, institutional) to deconstruction.\\n\"\n","        \"- maybe: Partial or contextual relevance (e.g., general C&D waste management \"\n","        \"that may touch on reuse).\\n\"\n","        \"- drop: No substantive connection to deconstruction, disassembly, or material reuse.\\n\\n\"\n","        \"Passage:\\n{text}\"\n","    )\n","\n","    def judge_one(text: str) -> Dict[str, Any]:\n","        \"\"\"Classify a single chunk as keep / maybe / drop.\"\"\"\n","        prompt = JUDGE_PROMPT_TEMPLATE.format(text=text[:4000])\n","        resp = call_with_retry(judge_llm.invoke, prompt)\n","        try:\n","            content = resp.content if hasattr(resp, \"content\") else str(resp)\n","            # Strip markdown code fences if present\n","            content = re.sub(r\"^```(?:json)?\\s*\", \"\", content.strip())\n","            content = re.sub(r\"\\s*```$\", \"\", content.strip())\n","            parsed_json = json.loads(content)\n","            if isinstance(parsed_json, list) and len(parsed_json) > 0 and isinstance(parsed_json[0], dict):\n","                # If it's a list containing dicts, take the first dict\n","                return parsed_json[0]\n","            elif isinstance(parsed_json, dict):\n","                return parsed_json\n","            else:\n","                # If it's neither a dict nor a list of dicts, default\n","                return {\"label\": \"maybe\", \"confidence\": 0.5, \"reason\": \"Malformed JSON response; defaulted.\"}\n","        except Exception:\n","            return {\"label\": \"maybe\", \"confidence\": 0.5, \"reason\": \"Non-JSON response; defaulted.\"}\n","\n","    total_lines = sum(1 for _ in open(in_path, \"r\", encoding=\"utf-8\"))\n","    judge_stats = {\"keep\": 0, \"maybe\": 0, \"drop\": 0}\n","\n","    with open(in_path, \"r\", encoding=\"utf-8\") as f_in, \\\n","         open(out_path, \"a\", encoding=\"utf-8\") as f_out:\n","        for line in tqdm(f_in, desc=\"Judging chunks\", total=total_lines):\n","            rec = json.loads(line)\n","            rec[\"judge\"] = judge_one(rec[\"text\"])\n","            label = rec[\"judge\"].get(\"label\", \"maybe\")\n","            judge_stats[label] = judge_stats.get(label, 0) + 1\n","            f_out.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n","\n","    print(\"Judged file:\", out_path)\n","    print(\"Label distribution:\", judge_stats)\n","else:\n","    print(\"USE_LLM_JUDGE=False or judge_llm not initialised; skipping.\")"],"outputs":[{"output_type":"display_data","data":{"text/plain":["Judging chunks:   0%|          | 0/364 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e7cb9d4a67e429c8690bc3e018d5bbc"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Judged file: /content/drive/MyDrive/ACTIVE/AU_deconstruction_domain/data_analysis/batch_enhanced_KG_outputs/outputs/kept_chunks/MASTER_kept_judged.jsonl\n","Label distribution: {'keep': 152, 'maybe': 205, 'drop': 7}\n"]}],"execution_count":9,"id":"tcz7PInAA_4h"},{"cell_type":"markdown","metadata":{"id":"CbsVllpPA_4i"},"source":["## 7) Runtime Guidance\n","\n","### Typical processing times for ~95 PDFs\n","\n","| Stage | Cost driver | Typical time |\n","|---|---|---|\n","| Text extraction (with OCR fallback) | I/O + PDF parsing + OCR on flagged pages | ~5-15 min |\n","| Embedding | Number of keyword-gated chunks x API latency | ~5-15 min |\n","| LLM judge | Number of kept chunks x judge model latency | ~10-30 min |\n","\n","### Speeding up\n","\n","- `USE_KEYWORD_GATE = True` reduces embedding volume by ~60-80%\n","- Increase `CHUNK_SIZE` for fewer, coarser chunks\n","- Lower `TOP_K_PER_QUERY` for fewer candidate matches\n","- `SKIP_ALREADY_PROCESSED = True` enables crash-safe resumability\n","- Pilot on 2-3 PDFs first, then scale\n","\n","### Resumability\n","\n","When `SKIP_ALREADY_PROCESSED = True`, the pipeline checks whether a\n","`<pdf>.kept.jsonl` already exists.  If so, that PDF is skipped.  You can\n","safely restart after a crash and processing continues from where it stopped.\n","\n","### Extraction quality flags\n","\n","Each page in the `full_text/*.pages.jsonl` files includes:\n","\n","- `extraction_method`: which engine produced the text (`pdfplumber`, `pymupdf`, `ocr_tesseract`)\n","- `fallback_attempted`: whether fallback engines were tried\n","- `likely_image_or_scan`: flagged `true` if text remains < 50 chars after all attempts\n","\n","These flags support targeted follow-up extraction (e.g., specialised table\n","parsing or manual review) on pages where automated extraction was insufficient."],"id":"CbsVllpPA_4i"}]}